import nltk
import re
import math
import numpy as np
from nltk.corpus import stopwords

# Define replacement patterns
R_patterns = [
    (r"won't", "will not"),
    (r"can't", "cannot"),
    (r"i'm", "i am"),
    (r"(\w+)'ll", r"\g<1> will"),
    (r"(\w+)'nt", r"\g<1> not"),
    (r"(\w+)'ve", r"\g<1> have"),
    (r"(\w+)'s", r"\g<1> is"),
    (r"(\w+)'re", r"\g<1> are"),
]

# Define the REReplacer class
class REReplacer:
    def __init__(self, patterns=R_patterns):
        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]

    def replace(self, text):
        s = text
        for (pattern, repl) in self.patterns:
            s = re.sub(pattern, repl, s)
        return s

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Importing the Data
file = open("C:/Users/Dell/Downloads/text.txt", "r")
text = file.read()

# Sentence Tokenization
nltk.download('punkt')
sents = nltk.sent_tokenize(text)

# Punctuation Removal and Word Replacement
sents_1 = []
rep_word = REReplacer()
for i in sents:
    n = rep_word.replace(i)
    n = n.translate(str.maketrans('', '', string.punctuation))
    sents_1.append(n)

# Word Tokenization
ws = []
for j in sents_1:
    sent = nltk.word_tokenize(j)
    for s in sent:
        ws.append(s)

# Stopwords Removal
texts = []
for w in ws:
    if w not in stop_words:
        texts.append(w)

# Finding Frequency and Probability of a word
vocabulary = []
for word in texts:
    if word not in vocabulary:
        vocabulary.append(word)
voc_len = len(vocabulary)
N = len(texts)

# Preparation for t-test
def T_test(w1, w2, c1, c2, c12):
    p_w1 = c1 / N
    p_w2 = c2 / N
    exp_mean = p_w1 * p_w2
    ob_mean = c12 / N
    s2 = ob_mean
    diff = ob_mean - exp_mean
    v = math.sqrt(s2 / N)
    t = diff / v
    t = round(t, 2)
    d = {}
    d['t-value'] = t
    d['word1'] = w1
    d['word2'] = w2
    return d

# Preparation for chi^2-test
def chi_2_test(w1, w2, c1, c2, c12):
    a = c12
    b = c1 - a
    cx = c2 - a
    r2 = N - c1
    d = r2 - cx
    rl = a + b
    e = a + cx
    f = b + d
    e_w1 = (rl * e) / N
    e_w2 = (rl * f) / N
    e_w3 = (e * r2) / N
    e_w4 = (f * r2) / N
    sum1 = (np.square(a - e_w1)) / e_w1
    sum2 = (np.square(b - e_w2)) / e_w2
    sum3 = (np.square(cx - e_w3)) / e_w3
    sum4 = (np.square(d - e_w4)) / e_w4
    c = {}
    c['chi-value'] = sum1 + sum2 + sum3 + sum4
    c['word1'] = w1
    c['word2'] = w2
    return c

# Perform statistical tests
ttest = []
chi2test = []
for i in range(len(texts) - 1):
    w1 = texts[i]
    w2 = texts[i + 1]
    w12 = w1 + " " + w2
    c1 = 0
    c2 = 0
    c12 = 0
    for w in range(len(texts) - 1):
        if texts[w] == w1:
            c1 += 1
            if texts[w + 1] == w2:
                c12 += 1
        if texts[w] == w2:
            c2 += 1
    ttest.append(T_test(w1, w2, c1, c2, c12))
    chi2test.append(chi_2_test(w1, w2, c1, c2, c12))

# Print results
print("t-test:\n")
for d in ttest:
    if d['t-value'] > 2.57:
        print("t-value:", d['t-value'], "\t Words:", d['word1'], "and", d['word2'], " are collocated!", end="\n")
    else:
        print("t-value:", d['t-value'], "\t Words:", d['word1'], "and", d['word2'], " are not collocated!", end="\n")

print("chi^2-test:\n")
for d in chi2test:
    if d['chi-value'] > 2.57:
        print("chi-value:", d['chi-value'], "\t Words:", d['word1'], "and", d['word2'], " are collocated!", end="\n")
    else:
        print("chi-value:", d['chi-value'], "\t Words:", d['word1'], "and", d['word2'], " are not collocated!", end="\n")

----------------------------------------------------------------
import nltk
import numpy as np
import pandas as pd
import string
import math

# Load Excel file
text = pd.read_excel('C:/Users/Dell/Downloads/Bank.xlsx', sheet_name='Sheet1')
text = text.drop(text.columns[0], axis=1)

# Create 'bank_def.txt'
df = pd.read_excel('C:/Users/Dell/Downloads/Bank.xlsx')
sentences_col = df.iloc[:, 0] # Column index 0
sense_col = df.iloc[:, 1] # Column index 1
grouped = df.groupby(df.columns[1])[df.columns[0]].apply(lambda x: '\n'.join(x)).reset_index()
sections = []
for sense, sentences in zip(grouped[df.columns[1]], grouped[df.columns[0]]):
    sections.append(sentences)
text_content = '\n\n\n'.join(sections)
with open('bank_def.txt', 'w') as f:
    f.write(text_content)

# Preprocessing for grouping
def df_conversion(defn, sense):
    lst = []
    temp = defn.split("\n")
    temp.remove(temp[0])
    for i in temp:
        for j in i.split(". "):
            j = j.translate(str.maketrans('', '', string.punctuation))
            lst.append(j)
    data = {"words": lst, "sense": [sense] * len(lst)}
    return pd.DataFrame(data)

# Split data into financial and river
with open('bank_def.txt', 'r') as f:
    text = f.read()
test_def, f_def, r_def = text.split("\n\n\n")

df1 = df_conversion(f_def, "Financial")
df2 = df_conversion(r_def, "River")
test_df = df_conversion(test_def, "?")
df1 = pd.concat([df1, df2])

# Display test data
test_df = test_df.drop(index=test_df.index[-2:])

# Extract unique words
def calculate_v(df):
    ll = df['words']
    l = []
    l1 = []
    for i in ll:
        l1.append(i.split())
    for i in l1:
        for j in i:
            if j not in l:
                l.append(j)
    return len(l)

# Filter sense and calculate probability
def prob_word(df, classifier, wrd, v):
    dff = df[df.sense == classifier]
    cnt = 20
    for word in dff['words']:
        if wrd in word:
            cnt += 1
    return (cnt + 1) / (len(dff) + v)

# Naive Bayes algorithm
def naive_bayes(df1, classifier, test_data):
    prob = prob_word(df1, classifier, "bank", calculate_v(df1))
    if prob <= 0:
        prob = 1e-10 # Small positive value to avoid log(0)
    prob_test = math.log2(prob)
    for word in test_data:
        word_prob = prob_word(df1, classifier, word, calculate_v(df1))
        if word_prob <= 0:
            word_prob = 1e-10 # Small positive value to avoid log(0)
        prob_test += math.log2(word_prob)
    return prob_test

# Display the test data result
sense_list = []
for i in test_df["words"]:
    print("\nFor Test_data: ", i)
    pb_r = naive_bayes(df1, 'River', i.split())
    pb_f = naive_bayes(df1, 'Financial', i.split())
    print("\tScore for p(test_data|River): ", pb_r)
    print("\tScore for p(test_data|Financial): ", pb_f)
    if pb_f > pb_r:
        sense_list.append("Financial")
        print("\tTest data comes under FI Classification")
    else:
        sense_list.append("River")
        print("\tTest data comes under River Border Classification")

-----------------------------------------------------------------
import math
from nltk.tokenize import word_tokenize
from nltk import bigrams

# Read the corpus from the file
with open('C:/Users/Dell/Downloads/corpus.txt', 'r') as file:
    corpus = file.read()

# Tokenize the corpus into words
tokens = word_tokenize(corpus.lower())

# Count the occurrences of elements and store in the dictionary
element_counts = {}
for element in tokens:
    element_counts[element] = element_counts.get(element, 0) + 1

# Generate bigrams from the tokenized words
bi_grams = list(bigrams(tokens))

# Count the frequency of each bigram
bigram_counts = {}
for bigram in bi_grams:
    bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1

# User input for noun, verb, and preposition
noun = input("Enter the Noun: ")
verb = input("Enter the Verb: ")
prep = input("Enter the Preposition: ")

# Calculate counts and probabilities
n = element_counts.get(noun, 0)
v = element_counts.get(verb, 0)
p_n = bigram_counts.get((prep, noun), 0)
p_v = bigram_counts.get((prep, verb), 0)

# Calculate probabilities and lambda
def cal_prob(p_v, p_n, v, n):
    prob_v = p_v / v if v > 0 else 0
    prob_n = p_n / n if n > 0 else 0
    return prob_v, prob_n

prob_v, prob_n = cal_prob(p_v, p_n, v, n)

def cal_lam(prob_v, prob_n):
    if prob_n == 0:
        prob_n = 1e-10  # Add a small smoothing value
    lambda_val = math.log((prob_v * (1 - prob_n)) / prob_n, 2)
    return lambda_val

# Example usage
prob_v = 0.8
prob_n = 0.0  # This would cause a division by zero without smoothing
lambda_val = cal_lam(prob_v, prob_n)
print(f"Lambda: {lambda_val}")

# Determine the attachment of the preposition
if lambda_val > 0:
    print("The Preposition is attached with Verb.")
elif lambda_val < 0:
    print("The Preposition is attached with Noun.")
else:
    print("The Preposition attachment cannot be determined.")

----------------------------------------------------------------
import nltk
import math
import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.util import bigrams

# Load data from CSV
df = pd.read_csv(r"C:\Users\Dell\Downloads\Attachment ambiguity WITH.csv", encoding='latin-1')
print(df.head())

# Concatenate all text into a single string
text = " ".join(df.astype(str).values.flatten())

# Tokenize the text
words = word_tokenize(text)

# Filter out punctuation
fil = [word for word in words if word not in [',', '.', '!', '?', '\'s']]

# Count word occurrences
count = Counter(fil)
print(count)
print(sum(count.values()))

# Generate bigrams and count their occurrences
big = list(bigrams(fil))
big_count = Counter(big)
print(big_count)

noun=input("Enter the Noun:")
verb=input("Enter the Verb:")
prep=input("Enter the preposition:")
c_noun=count[noun]
c_verb=count[verb]
c_np=big_count[(noun,prep)] +big_count[(prep,noun)]
print(c_np)
c_vp=big_count[(verb,prep)] + big_count[(prep,verb)]
print("count of noun:",c_noun)
print("count of verb:",c_verb)
print("pp with n:",c_np)
print("pp with v:",c_vp)
p_n=c_np/c_noun
print("Probability of noun:",p_n)
p_v=c_vp/c_verb
print("Probability of verb:",p_v)
p_vv=1- p_v
print("prob:",p_vv)
final_pro=(p_n * p_vv)/p_v
final_prob=np.log(final_pro)
print("FINAL:",final_prob)
if final_prob > 0:
 print("The Preposition is attached with Verb")
elif final_prob < 0:
 print("The Preposition is attached with Noun")
else:
 print("The Preposition attachment cannot be determined")
 
-----------------------------------------------------------------
import math

# Input Declarations
n = int(input("Enter the number of states: "))
prob = {}
state_name = {}
for i in range(n):
    prob[i] = float(input("Initial probability for state: "))
    s = input("State Name: ")
    state_name[i] = s

# Output Sequence
output = input("Output Sequence: ")
out = output.split()

# Emission States and Probabilities
op = input("Emission States: ")
op_emit = op.split()
emit = {}
for i in range(n):
    for j in op_emit:
        emit[i, j] = float(input(f"Emission Probability for state {i} to {j}: "))

# Transition Probabilities
transition = {}
for i in range(n):
    for j in range(n):
        transition[i, j] = float(input(f"Transition Probability from state {i} to {j}: "))

# Alpha and Beta Values
alpha = [[],]
beta = [[]]
for i in range(n):
    alpha[0].append(prob[i])
    beta[0].append(1)

# Forward Procedure
def trellis_fd(n, out, transition, emit, alpha):
    alp = []
    lists = []
    for i in range(n):
        for j in range(n):
            value = alpha[j] * emit[j, out] * transition[j, i]
            lists.append(value)
    if len(lists) == 2:
        alp.append(lists[0] + lists[1])
        lists = []
    elif len(lists) == 3:
        alp.append(lists[0] + lists[1] + lists[2])
        lists = []
    return alp

# Backward Procedure
def trellis_bd(n, out, transition, emit, beta):
    lists = []
    betal = []
    for i in range(n):
        for j in range(n):
            value = beta[j] * emit[i, out] * transition[i, j]
            lists.append(value)
    if len(lists) == 2:
        betal.append(lists[0] + lists[1])
        lists = []
    elif len(lists) == 3:
        betal.append(lists[0] + lists[1] + lists[2])
        lists = []
    return betal

# Trellis Forward Procedure
c = 0
for i in out:
    new = trellis_fd(n, i, transition, emit, alpha[c])
    alpha.append(new)
    c += 1

# Reverse List
def rev(lis):
    new_lst = lis[::-1]
    return new_lst
out = rev(out)

# Trellis Backward Procedure
c = 0
for i in out:
    new = trellis_bd(n, i, transition, emit, beta[c])
    beta.append(new)
    c += 1

# Probability of Forward and Backward Sequence
a = alpha[-1]
prob_fd_seq = 0
prob_bd_seq = 0
for i in a:
    prob_fd_seq += i
b = beta[-1]
c = alpha[0]
for i in range(len(b)):
    prob_bd_seq += (b[i] * c[i])
print(f"\n{prob_fd_seq}\n{prob_bd_seq}")

# Gamma Value Calculation
def best_seq(n, alpha, beta):
    if n == 2:
        num1 = alpha[0] * beta[0]
        num2 = alpha[1] * beta[1]
        total = num1 + num2
        gamma1 = num1 / total
        gamma2 = num2 / total
        return [gamma1, gamma2]
    elif n == 3:
        num1 = alpha[0] * beta[0]
        num2 = alpha[1] * beta[1]
        num3 = alpha[2] * alpha[2]
        total = num1 + num2 + num3
        gamma1 = num1 / total
        gamma2 = num2 / total
        gamma3 = num3 / total
        return [gamma1, gamma2, gamma3]

# Find Best Sequence
counter1 = 0
counter2 = 0
state0 = []
statel = []
state2 = []
gamma = []
while counter1 < len(alpha) and counter2 < len(beta):
    lists = []
    lists = best_seq(n, alpha[counter1], beta[counter2])
    gamma.append(lists)
    if n == 2:
        if lists[0] >= lists[1]:
            state0.append(lists[0])
            statel.append(0)
        elif lists[0] < lists[1]:
            statel.append(lists[1])
            state0.append(0)
    elif n == 3:
        if lists[0] > lists[1] and lists[0] > lists[2]:
            state0.append(lists[0])
            statel.append(0)
            state2.append(0)
        elif lists[0] < lists[1] and lists[2] < lists[1]:
            statel.append(lists[0])
            state0.append(0)
            state2.append(0)
        elif lists[0] < lists[2] and lists[1] < lists[2]:
            state2.append(lists[0])
            state0.append(0)
            statel.append(0)
        elif lists[0] == lists[1] and lists[1] == lists[2]:
            state0.append(lists[0])
            statel.append(0)
            state2.append(0)
    counter1 += 1
    counter2 += 1

# Display Sequence
j = 0
n1 = n
state = []
while j < len(gamma):
    if n1 == 2:
        if state0[j] > statel[j]:
            state.append(0)
        else:
            state.append(1)
    elif n1 == 3:
        if state0[j] > statel[j] and state0[j] > state2[j]:
            state.append(0)
        elif statel[j] > state0[j] and statel[j] > state2[j]:
            state.append(1)
        elif state2[j] > state0[j] and state2[j] > statel[j]:
            state.append(2)
    j += 1

state_display = []
for k in state:
    if k in state_name.keys():
        state_display.append(state_name[k])

print(f"\n{state_display}")
------------------------------------------------------------------
import math

# Input Declarations
print("Viterbi algorithmic procedure:\n")
n = int(input("Enter the number of states: "))
print("\nInitial probability for each state:\n")
prob = {}
state_name = {}
for i in range(n):
    prob[i] = float(input(f"Enter initial probability for state {i+1}: "))
    s = input(f"Enter the state name for state {i+1}: ")
    state_name[i] = s

# Output Sequence
output = input("Enter the output sequence (separated by spaces): ")
print("\nOutput sequence:", output)
out = output.split()

# Emission States and Values
print("\nState EMISSION schema:\n")
output1 = input("Enter the emission states: ")
print("\nOutput emissions:", output1)
out1 = output1.split()
emit = {}
for i in range(n):
    for j in out1:
        emit[i, j] = float(input(f"Enter the emission probability for state {state_name[i]} emitting {j}: "))
print(emit)

# Transition Probability
print("\nState transition schema:\n")
transition = {}
for i in range(n):
    for j in range(n):
        transition[i, j] = float(input(f"Enter the transition probability from state {state_name[i]} to {state_name[j]}: "))
print(transition)

# Delta Value
delta = [[]]
for i in range(n):
    delta[0].append(prob[i])
print(delta)

# Max Probability
init_prob_item = list()
state_list = []
for i in prob.values():
    init_prob_item.append(i)
m = max(init_prob_item)
v = init_prob_item.index(m)
for i in range(n):
    state_list.append(v)

# Maximum state
def max_state_addition(lists):
    value = max(lists)
    state_list.append(lists.index(value))

# Viterbi Algorithm
def viterbi(n, out, transition, emit, delta):
    maximum = []
    lists = []
    for i in range(n):
        for j in range(n):
            value = delta[j] * emit[j, out] * transition[j, i]
            lists.append(value)
        else:
            max_state_addition(lists)
            maximum.append(max(lists))
            lists = []
    return maximum

# Delta calculation
c = 0
for i in out:
    new = viterbi(n, i, transition, emit, delta[c])
    delta.append(new)
    c += 1
print(delta)

# Print Sequence
sequence_state1 = []
sequence_state2 = []
c = 0
if len(delta[0]) == 2:
    for i in delta:
        sequence_state1.append([i[0], state_list[c]])
        c += 1
        sequence_state2.append([i[1], state_list[c]])
        c += 1
    print(sequence_state1, "\n", sequence_state2)
elif len(delta[0]) == 3:
    for i in delta:
        sequence_state1.append([i[0], state_list[c]])
        c += 1
        sequence_state2.append([i[1], state_list[c]])
        c += 1
        sequence_state3.append([i[2], state_list[c]])
        c += 1
    print(sequence_state1, "\n", sequence_state2, "\n", sequence_state3)

# Find best sequence
best_seq_rev = []
count = len(sequence_state1)
if n == 2:
    for k in range(count):
        i = sequence_state1[k]
        j = sequence_state2[k]
        if i[0] > j[0]:
            best_seq_rev.append(i[1])
        elif i[0] < j[0]:
            best_seq_rev.append(j[1])
        elif i[0] == j[0]:
            best_seq_rev.append(i[1])
    print("\n\nProbability of output seq:", max(sequence_state1[-1][0], sequence_state2[-1][0]))
    print(best_seq_rev)
elif n == 3:
    for k in range(count):
        i = sequence_state1[k]
        j = sequence_state2[k]
        l = sequence_state3[k]
        if i[0] > j[0] and i[0] > l[0]:
            best_seq_rev.append(i[1])
        elif j[0] > i[0] and j[0] > l[0]:
            best_seq_rev.append(j[1])
        elif l[0] > i[0] and l[0] > j[0]:
            best_seq_rev.append(l[1])
        else:
            best_seq_rev.append(l[1]) # if all are equal
    print("\n\nProbability of output seq:", max(sequence_state1[-1][0], sequence_state2[-1][0], sequence_state3[-1][0]))
    print(best_seq_rev)

# Display State Names
state_display = []
for k in best_seq_rev:
    if k in state_name.keys():
        state_display.append(state_name[k])
state_display.reverse()
print(state_display)

----------------------------------------------------------------
import nltk
from nltk import PCFG, ViterbiParser
from nltk.tree import Tree

# Define a PCFG grammar
grammar = PCFG.fromstring("""
S -> NP VP [1.0]
VP -> V NP [0.7] | VP PP [0.3]
NP -> Det N [0.5] | NP PP [0.2] | 'John' [0.3]
PP -> P NP [1.0]
V -> 'saw' [0.2] | 'ate' [0.8]
Det -> 'the' [0.6] | 'a' [0.4]
N -> 'man' [0.5] | 'telescope' [0.5]
P -> 'with' [0.4] | 'in' [0.6]
""")

# Create a Viterbi parser
parser = ViterbiParser(grammar)

# Parse a sentence
sentence = "John saw a man with a telescope"
tokens = sentence.split()
for tree in parser.parse(tokens):
    print(tree)

# Another grammar
grammar = PCFG.fromstring("""
S -> NP VP [1.0]
PP -> P NP [1.0]
VP -> V NP [0.7] | VP PP [0.3]
P -> 'with' [1.0]
V -> 'saw' [1.0]
NP -> NP PP [0.4] | 'astronomers' [0.1] | 'ears' [0.18] | 'saw' [0.04]
| 'stars' [0.28]
""")

# Parse another sentence
parser = ViterbiParser(grammar)
sentence = "astronomers saw stars with ears"
tokens = sentence.split()
try:
    for tree in parser.parse(tokens):
        tree.pretty_print()
except:
    print("No Parse tree found")

-------------------------------------------------------------------
import pandas as pd
import numpy as np
import nltk
from nltk.grammar import PCFG
from nltk.parse import InsideChartParser

# Function to get user-defined grammar
def get_Grammar():
    print("Once done with the rule, type \"done\"")
    try:
        return PCFG.fromstring("\n".join(iter(lambda: input("Enter the Rule: ").strip(), "done")))
    except ValueError as e:
        print("Error in Grammar Rule: ", e)

# Function to parse and construct trees
def parse_Tree_Construct():
    pcfg = get_Grammar()
    parser = InsideChartParser(pcfg)
    sentence = input("Enter the Sentence: ").strip().split()
    parse = list(parser.parse(sentence))
    if not parse:
        print("No Parse Tree Found")
        return
    parse_data = [(str(tree), tree.prob()) for tree in parse]
    df = pd.DataFrame(parse_data, columns=["Parse Tree", "Probability"])
    print("\nMean Probability of Parse Tree:", np.mean(df["Probability"]))
    print("\nParse Tree Result:", df.head())
    for tree in parse:
        tree.pretty_print()
    return sentence, pcfg

# Run the parsing function
sentence, cnf_grammar = parse_Tree_Construct()

# CYK parsing function
def cyk_parse(sentence, grammar):
    n = len(sentence)
    table = np.empty((n, n), dtype=object)
    for i, word in enumerate(sentence):
        table[i, i] = [(p.lhs(), p.prob()) for p in grammar.productions() if len(p.rhs()) == 1 and p.rhs()[0] == word]
    for span in range(2, n + 1):
        for i in range(n - span + 1):
            table[i, i + span - 1] = [(p.lhs(), lp * rp * p.prob())
                                      for k in range(i, i + span - 1)
                                      for l, lp in (table[i, k] or [])
                                      for r, rp in (table[k + 1, i + span - 1] or [])
                                      for p in grammar.productions() if len(p.rhs()) == 2 and (l, r) == p.rhs()]
    return table

# Test the CYK parsing function
table = cyk_parse(sentence, cnf_grammar)
r, p = table[0][2][0]
if str(r) == 'S':
    print("Accepted -", p)
else:
    print("Not Accepted -", p)

# Viterbi parsing
from nltk import PCFG, ViterbiParser
viterbi_parser = ViterbiParser(cnf_grammar)
vparses = list(viterbi_parser.parse(sentence))
vparse_data = [(str(tree), tree.prob()) for tree in vparses]
vdf = pd.DataFrame(vparse_data, columns=["Parse Tree", "Probability"])
print("\nParse Results using Viterbi Parsing:\n", vdf)
vmean_prob = np.mean(vdf['Probability'])
print("Mean Probability of Parses:", vmean_prob)

-------------------------------------------------------------
